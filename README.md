# Passei Direto Study Case

Creation of BI architecture in a case study created by Passei Direto.
I made the whole architecture using free tools to not expose necessary data or unexpected expenses.

# Operation
I created a cloud airflow instance that can be accessed through this link with the data extraction flow

https://airflow-etl-manual.herokuapp.com/admin/airflow/tree?dag_id=Passei_Direto

> When accessing for the first time, it may take around 20 to 30 seconds to load the page as the machine may be at rest.

And the DW can be connected using these credentials (ex: DBeaver), it is limited by 10,000 lines, in the code I limit it to 500 inserts per dimension.
```
user = "hkmwrxkewkhzrh",
password = "a8f37ea49b38f2d3d07d853b15ed59e0a7b5edaea657c3450c970dd8b9038a57",
host = "ec2-54-91-178-234.compute-1.amazonaws.com",
port = "5432",
database = "de6eje61ar0ot3"
```
If you want to switch to a local bank, just enter your credentials in `` direct-pass-study-case / src / database_access / postgres_connect.py`` 'and run the DDL in the base found in `` direct-pass- study-case / sql / DDL / `` ''.

I used Google Drive as a lake, I created a gmail account for that. If you want to add data follow the credentials.
```
email: pdcasestudy@gmail.com
password: passeidireto2020
```

To run the local application just create a ** virtual python environment ** and install the dependencies with `` `pip``.
```
virtualenv .venv
source .venv/bin/activate
pip install -r requirements.txt
```
I tried to integrate a spark cluster from the free cloud databricks but without success, sparkJob can be run locally within the virtual environment.
``
{SPARK_PATH} / bin / spark-submit /src/spark_job.py
``
I also created a local shell-script to automate jobs, just run ```. / Src / automation.sh``` within the virtual environment.

Finally I uploaded an instance of ** streamlit ** to consume the data generated by spark and generate graphs and analysis.

https://secure-scrubland-89533.herokuapp.com
![alt text](https://github.com/JoaoVitorDeOliveira/passei-direto-study-case/blob/master/media/streamlit.png)


# DataWarehouse Logical and Physical Modeling
I made the design of the modeling with the program StarUML, based on the transactional files I used a SnowFlake modeling separating
the DataWarehouse in two areas, STAGE and Dimensional.

## Área Stage
![alt text](https://github.com/JoaoVitorDeOliveira/passei-direto-study-case/blob/master/media/MODELO_STAGE.jpg)

## Área Dimensional
![alt text](https://github.com/JoaoVitorDeOliveira/passei-direto-study-case/blob/master/media/MODELO_LOGICO.jpg)

The Stage area serves to bring the data as "raw" as possible to minimize possible failures in data intake
for the table, leave the transformation on the DW side and reduce work in the transactional bank.

With the data saved on the Stage, we were able to transform the data and take it to the dimensional models through optimized Querys using
Slow Changing Dimension, Casts and Coalesce.

The database used was Postgress installed on an EC2 by Heroku (I'll talk more about them later).

# DataStore
In this scenario, I decided to use Google drive as DataLake (to replace S3), separating it into 3 zones, ** Analitycs ** (with data processed by spark for consumption of visualization tools or for Data Scientists), ** Raw ** (where pure data comes from various sources, in this case events) and ** Transactional ** (where transactional data comes from CDC or replacing the transactional bank).

![alt text](https://github.com/JoaoVitorDeOliveira/passei-direto-study-case/blob/master/media/googledrive_lake.png)



# SparkJob
Job searches for data generated by user events and concatenates with DW data to understand the behavior and seek insights from the data, in this job I'm generating 3 CSV files and saving in the Drive's Analitycs zone, ** country.csv ** shows the number of users who are or are from other countries and who use the PD, ** full.csv ** shows the number of users by type and the last ** full.csv ** shows a complete dataset with concatenated event information with the transactional.


# Heroku and Airflow
> When accessing for the first time it may take about 20 to 30 seconds to load the page as the machine may be at rest

To make the analysis of the architecture easier, I used heroku services, they provide up to 5 applications and a free EC2 database with a limit of 10,000 saved records. To orchestrate data consumption and keep the database up to date, I'm using Airflow.


# Streamlit
To demonstrate how data in the analitycs zone can be consumed by a possible visualization tool or a data scientist, I am using Streamlit (an open source tool for data analysis and web publishing https://www.streamlit.io).


